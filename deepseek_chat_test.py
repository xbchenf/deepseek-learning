
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st
import re


with st.sidebar:
    st.markdown("## å·¦ä¾§èœå•æ ")
    # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨ 0 åˆ° 8192 ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º 8192ï¼ˆDeepSeek-R1-Distill-Qwen-7B æ”¯æŒ 128K ä¸Šä¸‹æ–‡ï¼Œå¹¶èƒ½ç”Ÿæˆæœ€å¤š 8K tokensï¼Œæˆ‘ä»¬æ¨èè®¾ä¸º 8192ï¼Œå› ä¸ºæ€è€ƒéœ€è¦è¾“å‡ºæ›´å¤šçš„Tokenæ•°ï¼‰
    max_length = st.slider("max_length", 0, 8192, 8192, step=1)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜
st.title("ğŸ’¬ DeepSeek R1 èŠå¤©æœºå™¨äºº")

# å®šä¹‰æ¨¡å‹è·¯å¾„
mode_name_or_path = '/root/autodl-tmp/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'

# æ–‡æœ¬åˆ†å‰²å‡½æ•°
def split_text(text):
    pattern = re.compile(r'<think>(.*?)</think>(.*)', re.DOTALL) # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    match = pattern.search(text) # åŒ¹é… <think>æ€è€ƒè¿‡ç¨‹</think>å›ç­”
  
    if match: # å¦‚æœåŒ¹é…åˆ°æ€è€ƒè¿‡ç¨‹
        think_content = match.group(1).strip() # è·å–æ€è€ƒè¿‡ç¨‹
        answer_content = match.group(2).strip() # è·å–å›ç­”
    else:
        think_content = "" # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ€è€ƒè¿‡ç¨‹ï¼Œåˆ™è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²
        answer_content = text.strip() # ç›´æ¥è¿”å›å›ç­”
  
    return think_content, answer_content


@st.cache_resource
def get_model():
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å– tokenizer
    tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–æ¨¡å‹ï¼Œå¹¶è®¾ç½®æ¨¡å‹å‚æ•°
    model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, torch_dtype=torch.bfloat16,  device_map="auto")

    return tokenizer, model

# åŠ è½½ Qwen2.5 çš„ model å’Œ tokenizer
tokenizer, model = get_model()


# å¦‚æœ session_state ä¸­æ²¡æœ‰ "messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ"}]

# éå† session_state ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ° session_state ä¸­çš„ messages åˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})

    # å°†å¯¹è¯è¾“å…¥æ¨¡å‹ï¼Œè·å¾—è¿”å›
    input_ids = tokenizer.apply_chat_template(st.session_state.messages,tokenize=False,add_generation_prompt=True)
    model_inputs = tokenizer([input_ids], return_tensors="pt").to('cuda')
    generated_ids = model.generate(model_inputs.input_ids,max_new_tokens=max_length)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    think_content, answer_content = split_text(response) # è°ƒç”¨split_textå‡½æ•°ï¼Œåˆ†å‰²æ€è€ƒè¿‡ç¨‹å’Œå›ç­”
    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ° session_state ä¸­çš„ messages åˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    with st.expander("æ¨¡å‹æ€è€ƒè¿‡ç¨‹"):
        st.write(think_content) # å±•ç¤ºæ¨¡å‹æ€è€ƒè¿‡ç¨‹
    st.chat_message("assistant").write(answer_content) # è¾“å‡ºæ¨¡å‹å›ç­”
# print(st.session_state) # æ‰“å° session_state è°ƒè¯• 